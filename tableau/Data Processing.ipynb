{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data\n",
    "orders_df = pd.read_csv('olist_orders_dataset.csv')\n",
    "order_items_df = pd.read_csv('olist_order_items_dataset.csv')\n",
    "products_df = pd.read_csv('olist_products_dataset.csv')\n",
    "customers_df = pd.read_csv('olist_customers_dataset.csv')\n",
    "sellers_df = pd.read_csv('olist_sellers_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data Transformation\n",
    "\n",
    "This part focuses on transforming the data into the appropriate structure, deriving relevant attributes, discarding irrelevant\n",
    "data, and establishing relations through primary and foreign keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform Data: -Restructure data into star schema format   -Create separate DataFrames for dimensions and facts\n",
    "\n",
    "# Date dimension\n",
    "date_dimension = pd.DataFrame({'date': pd.to_datetime(orders_df['order_purchase_timestamp']).dt.date.unique()})  # Ensure that date fields are parsed as datetime objects \n",
    "# Extract year, month, and day attributes from the order_purchase_timestamp\n",
    "date_dimension['year'] = pd.to_datetime(date_dimension['date']).dt.year\n",
    "date_dimension['month'] = pd.to_datetime(date_dimension['date']).dt.month\n",
    "date_dimension['month_name'] = pd.to_datetime(date_dimension['date']).dt.strftime('%B')\n",
    "date_dimension['day'] = pd.to_datetime(date_dimension['date']).dt.day\n",
    "date_dimension['date_key'] = np.arange(len(date_dimension))\n",
    "\n",
    "\n",
    "# 'xx_key' columns will serve as the PK for the dimension tables\n",
    "product_dimension = pd.DataFrame({'product_key': products_df['product_id']})  # 'product_key' column will serve as the PK for the product dimension\n",
    "seller_dimension = pd.DataFrame({'seller_key': sellers_df['seller_id']})\n",
    "customer_dimension = pd.DataFrame({'customer_key': customers_df['customer_id']})\n",
    "order_dimension = pd.DataFrame({'order_key': orders_df['order_id']})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with datasets dfs to include additional fields in the dimesnion tables:\n",
    "product_dimension = pd.merge(product_dimension, products_df, how='left', left_on='product_key', right_on='product_id')\n",
    "seller_dimension = pd.merge(seller_dimension, sellers_df, how='left', left_on='seller_key', right_on='seller_id')\n",
    "customer_dimension = pd.merge(customer_dimension, customers_df, how='left', left_on='customer_key', right_on='customer_id')\n",
    "order_dimension = pd.merge(order_dimension, orders_df, how='left', left_on='order_key', right_on='order_id')\n",
    "\n",
    "# Generate a manual key for order_items: // since we don't have unique ids in order_items\n",
    "order_items_df['order_item_unique_id'] = range(1, len(order_items_df) + 1)       \n",
    "# Assuming that 'order_item_id' column is acttually the 'quantity' so rename the field\n",
    "order_items_df.rename(columns={'order_item_id': 'quantity'}, inplace=True)\n",
    "# Drop duplicates and keep the row with the maximum quantity:\n",
    "order_items_df = order_items_df.sort_values(by=['order_id', 'product_id', 'quantity'], ascending=False) # first descendibg sort\n",
    "order_items_df.drop_duplicates(subset=['order_id', 'product_id'], keep='first', inplace=True)    # and then keep the first occurence = max and delete the other duplicate rows\n",
    "\n",
    "# Create the order_item dimension DataFrame\n",
    "order_item_dimension = pd.DataFrame({'order_item_key': order_items_df['order_item_unique_id']})\n",
    "order_item_dimension = pd.merge(order_item_dimension, order_items_df, how='left', left_on='order_item_key', right_on='order_item_unique_id')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Keep only the needed attributes in the dimension tables to answer the analytical question, and discard the others.\n",
    "order_dimension = order_dimension[['order_key', 'customer_id', 'order_purchase_timestamp']]\n",
    "# Convert order date in order_dimension to year-month-day format\n",
    "order_dimension['order_purchase_timestamp'] = pd.to_datetime(order_dimension['order_purchase_timestamp']).dt.date\n",
    "\n",
    "order_item_dimension = order_item_dimension[['order_item_key', 'order_id', 'product_id', 'seller_id', 'price', 'quantity']]\n",
    "# Rename the 'price' column to 'price_per_unit'\n",
    "order_item_dimension.rename(columns={'price': 'price_per_unit'}, inplace=True)\n",
    "\n",
    "\n",
    "product_dimension = product_dimension[['product_key', 'product_category_name']]\n",
    "customer_dimension = customer_dimension[['customer_key']]\n",
    "seller_dimension = seller_dimension[['seller_key', 'seller_state']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Data Integration\n",
    "\n",
    "This part involves connecting the datasets through shared attributes and establishing integration functions to connect entities if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect datasets through shared attributes (e.g., customer_id) (by merging DataFrames: merging the fact table with dimesnions tables)\n",
    "\n",
    "sales_fact = order_item_dimension\n",
    "\n",
    "\n",
    "# Merging order items with orders\n",
    "sales_fact = pd.merge(sales_fact, order_dimension, how='left', left_on='order_id', right_on='order_key') # We have to establish a relationship between the sales_fact df and the order_dimension df // here we are specifying the FKs\n",
    "\n",
    "\n",
    "\n",
    "# Merging with customers\n",
    "sales_fact = pd.merge(sales_fact, customer_dimension, how='left', left_on='customer_id', right_on='customer_key')\n",
    "\n",
    "\n",
    "\n",
    "# Merging with products\n",
    "sales_fact = pd.merge(sales_fact, product_dimension, how='left', left_on='product_id', right_on='product_key') \n",
    "\n",
    "\n",
    "\n",
    "# Merging with sellers\n",
    "sales_fact = pd.merge(sales_fact, seller_dimension, how='left', left_on='seller_id', right_on='seller_key')\n",
    "\n",
    "#column_names = sales_fact.columns\n",
    "#print(column_names)\n",
    "\n",
    "\n",
    "# Merge with date\n",
    "sales_fact = pd.merge(sales_fact, date_dimension, how='left', left_on='order_purchase_timestamp', right_on='date')\n",
    "\n",
    "\n",
    "\n",
    "# print(sales_fact.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Data Cleaning\n",
    "\n",
    "This part assesses if the data is subject to quality issues and resolves them where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with null order_key before removal: 0\n",
      "Number of entries with null product_key before removal: 0\n",
      "Number of entries with null product_category_name before removal: 1460\n",
      "Number of entries with null customer_key before removal: 0\n",
      "Number of entries with null seller_key before removal: 0\n",
      "Number of entries with null seller_state before removal: 0\n",
      "Number of entries with null price_per_unit before removal: 0\n",
      "Number of entries with null quantity before removal: 0\n",
      "Number of entries with null order_key after removal: 0\n",
      "Number of entries with null product_key after removal: 0\n",
      "Number of entries with null product_category_name after removal: 0\n",
      "Number of entries with null customer_key after removal: 0\n",
      "Number of entries with null seller_key after removal: 0\n",
      "Number of entries with null seller_state after removal: 0\n",
      "Number of entries with null price_per_unit after removal: 0\n",
      "Number of entries with null quantity after removal: 0\n"
     ]
    }
   ],
   "source": [
    "# Assess data quality and resolve issues\n",
    "\n",
    "# Check empty entries:\n",
    "print(\"Number of entries with null order_key before removal:\", sales_fact['order_key'].isnull().sum())\n",
    "print(\"Number of entries with null product_key before removal:\", sales_fact['product_key'].isnull().sum())\n",
    "print(\"Number of entries with null product_category_name before removal:\", sales_fact['product_category_name'].isnull().sum())\n",
    "print(\"Number of entries with null customer_key before removal:\", sales_fact['customer_key'].isnull().sum())\n",
    "print(\"Number of entries with null seller_key before removal:\", sales_fact['seller_key'].isnull().sum())\n",
    "print(\"Number of entries with null seller_state before removal:\", sales_fact['seller_state'].isnull().sum())\n",
    "print(\"Number of entries with null price_per_unit before removal:\", sales_fact['price_per_unit'].isnull().sum())\n",
    "print(\"Number of entries with null quantity before removal:\", sales_fact['quantity'].isnull().sum())\n",
    "\n",
    "# we don't check date_key and order_item_unique_id since we previously generated them manually\n",
    "\n",
    "# Remove rows where 'product_id' is null\n",
    "sales_fact = sales_fact.dropna(subset=['order_key', 'order_item_key', 'product_key', 'product_category_name', 'customer_key', 'date_key', 'seller_key', 'seller_state', 'price_per_unit', 'quantity']) # NO NULL product_id has been found!\n",
    "\n",
    "\n",
    "# Verify removal\n",
    "print(\"Number of entries with null order_key after removal:\", sales_fact['order_key'].isnull().sum())\n",
    "print(\"Number of entries with null product_key after removal:\", sales_fact['product_key'].isnull().sum())\n",
    "print(\"Number of entries with null product_category_name after removal:\", sales_fact['product_category_name'].isnull().sum())\n",
    "print(\"Number of entries with null customer_key after removal:\", sales_fact['customer_key'].isnull().sum())\n",
    "print(\"Number of entries with null seller_key after removal:\", sales_fact['seller_key'].isnull().sum())\n",
    "print(\"Number of entries with null seller_state after removal:\", sales_fact['seller_state'].isnull().sum())\n",
    "print(\"Number of entries with null price_per_unit after removal:\", sales_fact['price_per_unit'].isnull().sum())\n",
    "print(\"Number of entries with null quantity after removal:\", sales_fact['quantity'].isnull().sum())\n",
    "\n",
    "\n",
    "# Remove null entries\n",
    "order_item_dimension = order_item_dimension.dropna(subset=['order_item_key', 'order_id', 'product_id', 'seller_id', 'price_per_unit', 'quantity']) \n",
    "# make sure they're removed:\n",
    "#print(\"Number of entries with null quantity after removal:\", order_item_dimension['quantity'].isnull().sum())\n",
    "#print(\"Number of entries with null price_per_unit after removal:\", order_item_dimension['price_per_unit'].isnull().sum())\n",
    "order_dimension = order_dimension.dropna(subset=['order_key', 'customer_id', 'order_purchase_timestamp'])\n",
    "product_dimension = product_dimension.dropna(subset=['product_key', 'product_category_name']) \n",
    "date_dimension = date_dimension.dropna(subset=['date_key', 'date'])\n",
    "customer_dimension = customer_dimension.dropna(subset=['customer_key']) \n",
    "seller_dimension = seller_dimension.dropna(subset=['seller_key', 'seller_state']) \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [order_item_key, order_id, product_id, seller_id, price_per_unit, quantity, order_key, customer_id, order_purchase_timestamp, customer_key, product_key, product_category_name, seller_key, seller_state, date, year, month, month_name, day, date_key]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows based on key ids:\n",
    "duplicates = sales_fact.duplicated(subset=['order_key', 'order_item_key', 'customer_key', 'product_key'], keep='first')  # remove duplicate rows where it's the same order same odrer_item by the same customer and same product simultaneously.\n",
    "print(sales_fact[duplicates]) # must be empty (0 duplicates) since we already kept the rows with the max quantity in the part of Data Transformation\n",
    "\n",
    "\n",
    "# Filter out the duplicates from the df:\n",
    "sales_fact = sales_fact[~duplicates]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Export:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the Transformed and cleaned data to CSV files = Dimensions tables + Fact table\n",
    "\n",
    "# Keep only the keys in fact table:\n",
    "sales_facts = sales_fact[['order_key', 'order_item_key', 'product_key', 'customer_key', 'date_key', 'seller_key']]\n",
    "sales_facts.to_csv('Output/sales_fact.csv', index=False)\n",
    "\n",
    "# Dimensions have the needed attributes\n",
    "order_dimension.to_csv('Output/order_dimension.csv', index=False)\n",
    "order_item_dimension.to_csv('Output/orderitem_dimension.csv', index=False)\n",
    "product_dimension.to_csv('Output/product_dimension.csv', index=False)\n",
    "customer_dimension.to_csv('Output/customer_dimension.csv', index=False)\n",
    "seller_dimension.to_csv('Output/seller_dimension.csv', index=False)\n",
    "date_dimension.to_csv('Output/date_dimension.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null values in each column in sales_fact:\n",
      "order_key         0\n",
      "order_item_key    0\n",
      "product_key       0\n",
      "customer_key      0\n",
      "date_key          0\n",
      "seller_key        0\n",
      "dtype: int64\n",
      "Number of null values in each column in order_dimension:\n",
      "order_key                   0\n",
      "customer_id                 0\n",
      "order_purchase_timestamp    0\n",
      "dtype: int64\n",
      "Number of null values in each column in orderitem_dimension:\n",
      "order_item_key    0\n",
      "order_id          0\n",
      "product_id        0\n",
      "seller_id         0\n",
      "price_per_unit    0\n",
      "quantity          0\n",
      "dtype: int64\n",
      "Number of null values in each column in product_dimension:\n",
      "product_key              0\n",
      "product_category_name    0\n",
      "dtype: int64\n",
      "Number of null values in each column in customer_dimension:\n",
      "customer_key    0\n",
      "dtype: int64\n",
      "Number of null values in each column in seller_dimension:\n",
      "seller_key      0\n",
      "seller_state    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Make sure again we don't have any null value in all tables!\n",
    "file_path = \"Output/sales_fact.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "print(\"Number of null values in each column in sales_fact:\")\n",
    "print(null_counts)\n",
    "\n",
    "\n",
    "file_path = \"Output/order_dimension.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "print(\"Number of null values in each column in order_dimension:\")\n",
    "print(null_counts)\n",
    "\n",
    "file_path = \"Output/orderitem_dimension.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "print(\"Number of null values in each column in orderitem_dimension:\")\n",
    "print(null_counts)\n",
    "\n",
    "file_path = \"Output/product_dimension.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "print(\"Number of null values in each column in product_dimension:\")\n",
    "print(null_counts)\n",
    "\n",
    "file_path = \"Output/customer_dimension.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "print(\"Number of null values in each column in customer_dimension:\")\n",
    "print(null_counts)\n",
    "\n",
    "file_path = \"Output/seller_dimension.csv\" \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "print(\"Number of null values in each column in seller_dimension:\")\n",
    "print(null_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
